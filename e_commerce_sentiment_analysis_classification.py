# -*- coding: utf-8 -*-
"""E commerce Sentiment Analysis/Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kj9RqP40RlfrUltIbCDPZa2ZcPKZqc4h
"""

import pandas as pd
import pandas as pd
import numpy as np
import sklearn as sk
import matplotlib.pyplot as plt
import seaborn as sns

# Read the CSV file into a DataFrame
dataset = pd.read_csv("Womens Clothing E-Commerce Reviews.csv")

dataset.head()

dataset.isnull().sum()

dataset.info()

dataset.describe()

dataset.describe(include= 'all')

dataset.shape

sns.histplot(dataset.Age)
plt.title("Age Distribution")
plt.show()

plt.figure(figsize= (10, 5))
plt.title("Recommendation Based on Age")
sns.histplot(x= "Age", hue="Recommended IND", data = dataset)
plt.show()

dataset["Recommended IND"].value_counts()

sns.countplot(data=dataset, y="Recommended IND", orient="h")
plt.xlabel("Count")
plt.ylabel("Recomendation Level")
plt.title("Distribution of Recommendation Levels")
plt.show()

import pandas as pd

# Assuming 'dataset' is your existing DataFrame containing the data
# Create a new DataFrame with the desired features
dataset = dataset[['Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Age']].copy()

# Display the new DataFrame
print(dataset.head())

# Print unique values in the specified columns
print("Unique values in 'Division Name':", dataset['Division Name'].unique())
print("Unique values in 'Department Name':", dataset['Department Name'].unique())
print("Unique values in 'Class Name':", dataset['Class Name'].unique())

from sklearn.preprocessing import LabelEncoder

# Replace NaN values with a placeholder or handle them appropriately
# For now, let's replace NaN with a string 'Unknown'
dataset['Division Name'].fillna('Unknown', inplace=True)
dataset['Department Name'].fillna('Unknown', inplace=True)
dataset['Class Name'].fillna('Unknown', inplace=True)

# Apply label encoding to the specified columns
label_encoder = LabelEncoder()
dataset['Division Name'] = label_encoder.fit_transform(dataset['Division Name'])
dataset['Department Name'] = label_encoder.fit_transform(dataset['Department Name'])
dataset['Class Name'] = label_encoder.fit_transform(dataset['Class Name'])

# Print the unique values in each column after label encoding
print("Unique values in 'Division Name':", dataset['Division Name'].unique())
print("Unique values in 'Department Name':", dataset['Department Name'].unique())
print("Unique values in 'Class Name':", dataset['Class Name'].unique())

dataset.head()

corr_matrix = dataset.corr()
fig, ax = plt.subplots(figsize=(12, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="OrRd")

plt.figure(dpi=200)
sns.pairplot(dataset, hue='Recommended IND')
plt.show()

from sklearn.model_selection import train_test_split

X = dataset.drop('Recommended IND',axis=1)
y = dataset['Recommended IND']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y, random_state=0)

from imblearn.over_sampling import RandomOverSampler

resampler = RandomOverSampler(random_state=0)
X_train_oversampled, y_train_oversampled = resampler.fit_resample(X_train, y_train)

sns.countplot(x = y_train_oversampled)

from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
print(y_pred)

print(y_test)

from sklearn import metrics
acc = metrics.accuracy_score(y_test, y_pred)
print('accuracy:%.2f\n\n'%(acc))
cm=metrics.confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(cm, '\n\n')
print('--------------------------------')
result= metrics.classification_report(y_test, y_pred)
print('Classification Report: \n')
print(result)

ax = sns.heatmap(cm, cmap = 'flare', annot = True, fmt = 'd')

plt.xlabel("Predicted Class", fontsize = 12)
plt.ylabel("True Class", fontsize = 12)
plt.title("Confusion Matrix", fontsize = 12)

plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score
from joblib import dump



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


rf = RandomForestClassifier()


param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')


grid_search.fit(X_train, y_train)


best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_


y_pred = best_estimator.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Set: {accuracy}")

from sklearn.model_selection import cross_val_score

# Define the RandomForestClassifier model with the best parameters
rf = RandomForestClassifier(**best_params)

# Perform cross-validation
cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation score:", cv_scores.mean())

from sklearn.ensemble import RandomForestClassifier

# Assuming you have already defined your features and target variables

# Create a RandomForestClassifier model
rf_model = RandomForestClassifier()

# Fit the model to your data
rf_model.fit(X_train, y_train)

# Get feature importances from the trained model
feature_importances = rf_model.feature_importances_

# Assuming you have defined feature_names as a list of feature names
# Print feature importances
print("Feature Importances:")
for i, (feature_name, importance) in enumerate(zip(feature_names, feature_importances), 1):
    print(f"Feature {i}: {feature_name} - Importance: {importance}")

from sklearn.model_selection import train_test_split

X = dataset.drop('Recommended IND',axis=1)
y = dataset['Recommended IND']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y, random_state=0)

from imblearn.over_sampling import RandomOverSampler

resampler = RandomOverSampler(random_state=0)
X_train_oversampled, y_train_oversampled = resampler.fit_resample(X_train, y_train)

sns.countplot(x = y_train_oversampled)

from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data
X_test_scaled = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p = 2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
print(y_pred)

print(y_test)

from sklearn import metrics
acc = metrics.accuracy_score(y_test, y_pred)
print('accuracy:%.2f\n\n'%(acc))
cm=metrics.confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(cm, '\n\n')
print('--------------------------------')
result= metrics.classification_report(y_test, y_pred)
print('Classification Report: \n')
print(result)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


rf = RandomForestClassifier()


param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

grid_search.fit(X_train, y_train)


best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_


y_pred = best_estimator.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Set: {accuracy}")

from sklearn.model_selection import cross_val_score

# Define the RandomForestClassifier model with the best parameters
rf = RandomForestClassifier(**best_params)

# Perform cross-validation
cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation score:", cv_scores.mean())



# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import seaborn as sns
from wordcloud import WordCloud
import nltk
nltk.download(['stopwords',
               'punkt',
               'wordnet',
               'omw-1.4',
               'vader_lexicon'
               ])
# %matplotlib inline

# Read the CSV file
data = pd.read_csv('/content/Womens Clothing E-Commerce Reviews.csv')

# Select the columns you want to work with and rename them
data = data[['Recommended IND', 'Review Text', 'Department Name',]]  # Replace 'ColumnName1' and 'ColumnName2' with the actual column names
data.columns = ['Target_Label', 'Review Text', 'Department Name']  # Rename the selected columns

data.head()

print('\n ALL Data Labels')
print(data.groupby('Target_Label').count())

missing_values = data['Review Text'].isnull().sum()

print(f"Number of missing values in 'Review Text': {missing_values}")

# Delete rows with missing values in 'Review Text' column
data.dropna(subset=['Review Text', 'Department Name'], inplace=True)

# Count missing values after deletion
missing_values_after = data['Review Text'].isnull().sum()
print(f"Number of missing values in 'Review Text' after deletion: {missing_values_after}")

data.head()

tokenizer = nltk.tokenize.RegexpTokenizer('[a-zA-z0-9\']+')

# Convert 'Review Text' column to string before tokenizing
data['Review Text'] = data['Review Text'].astype(str)

# Tokenize the 'Review Text' column
data['tokenized_review'] = data['Review Text'].apply(lambda x: tokenizer.tokenize(x))

stop_words = nltk.corpus.stopwords.words('english')

# Applying lowercasing to individual words within the tokenized lists
data['cleaned_review'] = [
    [word.lower() for word in token_list if word.lower() not in stop_words]
    for token_list in data['tokenized_review']
]

import nltk
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

data['lemmatized_review'] = [
    [lemmatizer.lemmatize(word) for word in token_list]
    for token_list in data['cleaned_review']
]

data.head()

from nltk.sentiment.vader import SentimentIntensityAnalyzer



sentiment= SentimentIntensityAnalyzer()

data['compound'] = [sentiment.polarity_scores(" ".join(review))['compound'] for review in data['lemmatized_review']]
data['neg'] = [sentiment.polarity_scores(" ".join(review))['neg'] for review in data['lemmatized_review']]
data['neu'] = [sentiment.polarity_scores(" ".join(review))['neu'] for review in data['lemmatized_review']]
data['pos'] = [sentiment.polarity_scores(" ".join(review))['pos'] for review in data['lemmatized_review']]

data.head()

data[['compound','neg','neu','pos']].describe()

sns.histplot(data['compound'])

sns.histplot(data['pos'])

sns.histplot(data['neg'])

sns.histplot(data['neu'])

# count of negative reviews per product
(data['neg']<=0).groupby(data['Department Name']).sum()

percent_negative = pd.DataFrame((data['compound']<=0).groupby(data['Department Name']).sum()
                                /data['Department Name'].groupby(data['Department Name']).count()*100,
                                columns=['% negative reviews']).sort_values(by='% negative reviews')


percent_negative

sns.barplot(data=percent_negative, x='% negative reviews', y=percent_negative.index, color='c')


plt.show()

# Filtering positive reviews for specific classes ('Trend) with compound score > 0
reviews_positive_subset = data.loc[(data['Department Name']=='Dresses' )
                                  & (data['compound'] > 0)]

# Filtering negative reviews for specific classes (Trend) with compound score <= 0
reviews_negative_subset = data.loc[ (data['Department Name']=='Dresses' )
                                  & (data['compound'] <= 0)]


reviews_positive_subset.head()

from nltk.probability import FreqDist
from wordcloud import WordCloud
import matplotlib.pyplot as plt

neg_tokens = [word for review in reviews_negative_subset['lemmatized_review'] for word in review]

wordcloud = WordCloud(background_color='white').generate_from_text(
    ' '.join(neg_tokens))

plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

from nltk.probability import FreqDist
from wordcloud import WordCloud
import matplotlib.pyplot as plt

pos_tokens = [word for review in reviews_positive_subset['lemmatized_review'] for word in review]

wordcloud = WordCloud(background_color='white').generate_from_text(
    ' '.join(pos_tokens))

plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

from nltk.probability import FreqDist

pos_Freqdist = FreqDist(pos_tokens)

pos_Freqdist.tabulate(10)

from nltk.probability import FreqDist

neg_Freqdist = FreqDist(neg_tokens)

neg_Freqdist.tabulate(10)

pos_Freqdist.plot(30)

neg_Freqdist.plot(30)

# Categorizing sentiment based on thresholds
data['sentiment_label'] = 'Neutral'
data.loc[data['compound'] > 0.1, 'sentiment_label'] = 'Positive'
data.loc[data['compound'] < -0.1, 'sentiment_label'] = 'Negative'

# Count the number of reviews in each sentiment category
sentiment_counts = data['sentiment_label'].value_counts()
print(sentiment_counts)

# the average sentiment score: summary of the sentiment by aggregating scores per department
average_sentiment = data.groupby('Department Name')['compound'].mean()
print(average_sentiment)

